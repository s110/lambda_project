# Scraper Serverless de Datos Inmobiliarios en AWS

Este proyecto implementa un sistema de web scraping automatizado, escalable y eficiente en costos para extraer datos de propiedades inmobiliarias del sitio web `infocasas.com.pe`. La arquitectura est√° completamente basada en servicios serverless de AWS, lo que garantiza un bajo mantenimiento y un modelo de pago por uso.

## üéØ Objetivo del Proyecto

El objetivo principal es recopilar, enriquecer y almacenar informaci√≥n detallada de propiedades en venta (precio, ubicaci√≥n, caracter√≠sticas, coordenadas geogr√°ficas, etc.) de forma peri√≥dica y autom√°tica. Los datos recopilados se almacenan en una base de datos NoSQL (Amazon DynamoDB), dej√°ndolos listos para ser consumidos por otros servicios, como an√°lisis de datos, APIs o paneles de visualizaci√≥n.

## üèõÔ∏è Arquitectura de la Soluci√≥n

La soluci√≥n utiliza un patr√≥n de dise√±o **"Fan-Out"** para desacoplar las tareas y permitir un procesamiento paralelo y resiliente. Esto evita los timeouts de las funciones Lambda y distribuye la carga de trabajo de manera eficiente.


### Flujo de Trabajo:

1.  **Disparo Programado**: Un disparador de **Amazon EventBridge** se activa seg√∫n una programaci√≥n definida (p. ej., una vez al d√≠a).
2.  **Orquestaci√≥n**: El disparador invoca una funci√≥n **AWS Lambda ("Orchestrator")**. Esta funci√≥n es responsable de:
    *   Navegar por las p√°ginas de listado del sitio web.
    *   Extraer los datos b√°sicos de cada propiedad.
    *   Enviar cada propiedad como un mensaje individual a una cola de **Amazon SQS**.
3.  **Cola de Tareas**: La cola de **Amazon SQS** act√∫a como un b√∫fer, almacenando de forma fiable todas las "tareas" (propiedades a procesar) pendientes.
4.  **Procesamiento en Paralelo**: La cola SQS activa una segunda funci√≥n **AWS Lambda ("Worker")** por cada lote de mensajes. Gracias a la escalabilidad de Lambda, m√∫ltiples workers pueden ejecutarse en paralelo. Cada worker:
    *   Recibe un mensaje con los datos de una propiedad.
    *   Realiza una segunda petici√≥n al sitio web para visitar la p√°gina de detalle y extraer informaci√≥n adicional (en este caso, las coordenadas geogr√°ficas).
    *   Limpia y formatea el registro completo para que sea compatible con la base de datos.
5.  **Almacenamiento**: El worker guarda el registro final y enriquecido como un √≠tem en una tabla de **Amazon DynamoDB**.

### Ventajas de esta Arquitectura:
*   **Escalabilidad**: El sistema puede manejar desde unas pocas hasta miles de propiedades sin cambios en la arquitectura.
*   **Eficiencia en Costos**: Gracias a la capa gratuita de AWS y al modelo de pago por uso, el costo de operaci√≥n es pr√°cticamente nulo para vol√∫menes bajos y muy bajo para vol√∫menes altos.
*   **Resiliencia**: Si el scraping de una p√°gina de detalle falla, solo afecta a un mensaje en la cola, que puede ser reintentado autom√°ticamente sin detener todo el proceso.
*   **Bajo Mantenimiento**: No hay servidores que gestionar, parchear o monitorizar.

## üõ†Ô∏è Componentes del C√≥digo

El repositorio est√° organizado en dos partes principales, correspondientes a cada funci√≥n Lambda:

### 1. `lambda_orchestrator.py`
*   **Responsabilidad**: Scrapear las p√°ginas de listado.
*   **Librer√≠as Clave**: `httpx` para las peticiones HTTP, `BeautifulSoup4` para el parseo de HTML y `boto3` para enviar mensajes a SQS.

### 2. `lambda_worker.py`
*   **Responsabilidad**: Recibir tareas de SQS, scrapear p√°ginas de detalle y guardar en DynamoDB.
*   **Librer√≠as Clave**: `httpx`, `BeautifulSoup4`, y `boto3` para interactuar con DynamoDB. Incluye l√≥gica para limpiar los datos y manejar tipos num√©ricos (`Decimal`) para la compatibilidad con DynamoDB.

## üöÄ C√≥mo Desplegar

Para desplegar esta soluci√≥n en tu propia cuenta de AWS, sigue estos pasos:

1.  **Prerrequisitos**:
    *   Una cuenta de AWS.
    *   Python 3.9+ y `pip` instalados localmente.
    *   Tener configurada la [AWS CLI](https://aws.amazon.com/cli/) (opcional, pero recomendado).

2.  **Crear Recursos de AWS**:
    *   **DynamoDB**: Crea una tabla con una clave de partici√≥n de tipo String llamada `id`.
    *   **SQS**: Crea una cola est√°ndar. Copia su URL.
    *   **IAM Role**: Crea un rol para Lambda con permisos para escribir en CloudWatch Logs (`AWSLambdaBasicExecutionRole`), acceso completo a SQS (`AmazonSQSFullAccess`) y a DynamoDB (`AmazonDynamoDBFullAccess`).
      *(Nota: En entornos restrictivos como AWS Academy, puede que necesites usar un rol pre-configurado como `LabRole`)*.

3.  **Empaquetar las Dependencias**:
    *   Para cada funci√≥n (`orchestrator` y `worker`), crea una carpeta e instala las dependencias dentro de ella:
      ```bash
      # Ejemplo para el orquestador
      mkdir orchestrator_pkg
      cd orchestrator_pkg
      pip install httpx beautifulsoup4 boto3 -t .
      cp ../lambda_orchestrator.py ./lambda_function.py
      ```
    *   Comprime el contenido de cada carpeta en un archivo `.zip`, asegur√°ndote de que `lambda_function.py` quede en la ra√≠z del zip.

4.  **Desplegar las Funciones Lambda**:
    *   Crea dos funciones Lambda en la consola de AWS.
    *   Sube el `.zip` correspondiente a cada una.
    *   Asigna el rol de IAM creado.
    *   Configura las **variables de entorno**:
        *   Para `scraper-orchestrator`: `SQS_QUEUE_URL` con la URL de tu cola.
        *   Para `scraper-worker`: `DYNAMODB_TABLE_NAME` con el nombre de tu tabla.
    *   Ajusta los **tiempos de espera (timeout)** (ej. 5 min para el orquestador, 1 min para el worker).

5.  **Configurar los Disparadores (Triggers)**:
    *   Para `scraper-orchestrator`, a√±ade un disparador de **EventBridge** con una regla de programaci√≥n.
    *   Para `scraper-worker`, a√±ade un disparador de **SQS** apuntando a tu cola.

¬°Y listo! Ya puedes lanzar una prueba manual desde la Lambda del orquestador para verificar que todo el flujo funciona correctamente.
